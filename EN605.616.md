---
title: EN.605.616.81.SP23
description: The JHU course Multiprocess Architecture & Programming.
---

# EN.605.616.81.SP23: Multiprocess Architecture & Programming

## Module One

### Notes:

Reading: 
  Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
  Chapter 1: Introduction
  
_Mutual Exclusion_ is the problem or property that only one entity is allowed in a critical section. I.e. access is mutually exclusive. Necessary properties of mutual exclusion are _deadlock-free_ and _lockout-freedom_. Deadlock-free is defined as an entity that wants to enter a critical section eventually succeeds. If two entities want to enter the critical section then one of them eventually succeeds. Starvation-freedom is the property that if an entity wants to enter the critical section then it will eventually do so. The final property that is useful for mutual exclusion is _waiting_. Waiting is that if two entities desire access to the critical section and one of them get access, the other will wait until the one with access finishes.

The _producer-consumer_ problem is when there are two entities. One entity supplies some resource and another consumes the resource. The problem is coordinating the interaction between the two entities such that the consumer only accesses the resources when the producer has made them available. Additionally, the producing entity will not produce more resource than can be consumed. Notes that mutual exclusion cannot be used for the producer-consumer module as the consumer needs to be free to "check" for the resource as often as desired. 

_Amdahl's Law_ captures the idea of how much speed up we can get through single-level parallelization. The maxiumum speedup $S$ that can be acchived by $n$ processors collaborating on some application where $p$ is the fraction of the application that is parallelizable. Amdahl's Law states that the speed up by adding a parallel portion $\frac{p}{n}$ is:

$$
S = \frac{1}{1 - p + \frac{p}{n}}
$$

Note that this does not account for different levels of values of parallelization. There is strictly a single sequential portion and single parallelizable prortion. For an extention to Amdahl's Law that includes multi-level parallelism (i.e. processes and threads) read [Speedup for Multi-level Parallel Computing](https://www.comp.nus.edu.sg/~hebs/pub/shangjianghips12.pdf).

From the book we have the following excerpt on Amdahl's Law:
```
In general, however, for a given problem and a ten-processor machine,
Amdahl’s Law says that even if we manage to parallelize 90% of the solution,
but not the remaining 10%, then we end up with a five-fold speedup, but not
a ten-fold speedup. In other words, the remaining 10% that we did not paral-
lelize cut our utilization of the machine in half. 
```

  
### Discussion Board Posts

- What are the differences for the requirements between mutual exclusion mechanisms and producer/consumer mechanisms in parallel computing?
  - Mutual exclusion is controlled access to some critical section. Mutual exclusion is not necessarily about resources. However, the producer-consumer patter is about exposing resources. In the mutual exclusion pattern we require that if there are some entities that desire access to a resource or critical section then at least one of them succeeds. Note that this means there may be $n-1$ entities that never get access the resource or critical section. That is they "starve". However, in the producer-consumer pattern we have the requirement that any entity that wants to access the resource or critical section then it will succeed. That is if there are $n$ entities that require access to a resource or critical secton then $n$ entites will eventually be granted access.
  - The most obvious difference is that producer-consumer has the more strict requirement of starvation-free.

- Can we use Amdahl’s law more generically? For example, there is a software that will be executed in a 10-processor computer. The software has 20% of it can only be run in sequential, 30% of it can be run concurrently in 5 processors, and the remaining 50% of it can be run concurrently in all 10 processors. What is the speedup for this execution comparing to a sequential execution?
  - I was unable to find an extension or generalization of _Amdahl's Law_ that includes different percentages of parallelization across processors as described in the question. However, I was able to find a paper ([Speedup for Multi-Level Parallel Computing](https://www.comp.nus.edu.sg/~hebs/pub/shangjianghips12.pdf) which described an extension to Amdahl's Law that map much better to my experience in parallel computing. That is, it describes an extension to Amdahl's Law that accounts for multi-level parallelism afforded in modern computing environments. Specifically, the mutli-level here refers to process level parallelism combined with thread level parallelism. In distributed computing you may have an application that runs the same process, with shared memory, over some number of nodes. Assume for simplicity that there is one process per node. Then we have processes $p$ equals the number of nodes $n$. For each process we have another level of parallelism at the thread level. That is for each process $p_{i}$, we have $t_{i}$ threads for parallelism at the second level. More accurately, the thread-level parallelism is concurrency and is defined by $\alpha : 0 \leq \alpha \leq 1$. The multi-process portion is parallelization $\beta : 0 \leq \beta \leq 1$. The we have the multi-level speedup as:
```math
sp(\alpha,\beta,p,t) = sp(i = 1) = \frac{1}{1 - \alpha + \frac{\alpha(1 - \beta + \frac{\beta}{t})}{p}}
```

- The asynchrony of processor (or thread) executions is bad for parallel computing. Why is that? Explain your opinions.
  - The asynvhony of a process or thread execution is bad because it conflict with our intuitions of how source code and the resulting assembly ought to run. Specifically, this affects our intitions of around [sequential consistency](https://www.educative.io/answers/what-is-sequential-consistency-in-distributed-systems). For example, in the case of threads, we may not have insight in to how the operating system will schedule the programs threads. In fact we should program in such a way that we do not rely on thread execution ording. At the processor, the CPU may [reorder memory operations](https://en.wikipedia.org/wiki/Memory_ordering). Such reordering can be, and is done, by the compiler in languages such as C. At the network level, we may do some network transaction and on large systems there may be no guaranteed delivery ordering. 
  - The discrepancy between how we think about ordering and the systems we depend on to compile and execute our programs lead to inefficiencies, bugs, and vulnerabilities. 
  - There are tools to enforce ordering in each of these contexts. In the thread and processer contexts we can use serialized instructions [serialize](https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-isa-instructions/serialize/serialize-1.html) and [atomic instructions](https://www.ibm.com/docs/en/aix/7.2?topic=services-atomic-operations). In shared memory and networking context we can use function calls such as _quiet_ and _fence_ which enforce different constraints on delivery of network traffic [see: SHMEM 1.5](http://www.openshmem.org/site/sites/default/site_files/openshmem-1.5rc1.pdf). 
    
## Module Two

### Notes 
Reading: 
  Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
  Chapter 2

Definitions:
- _Instantaneous_: An event is instantaneous if it occurs within a single time unit. 
- _Preceeds_: an event $\alpha$ preceeds another event $\beta$, denoted $\alpha \prec \beta$, if $\alpha$ occurs at a time prior to $\beta$.
- _Interval_: an interval a $(\alpha_{0}, \alpha_{1})$ is the number of time units between event $\alpha_{0}$ and $\alpha_{1}$.
- _Concurrent_: Intervals $(\alpha_{0}, \alpha_{1})$ and $(\beta_{0}, \beta_{1})$ that do not have the preceeds relation, $(\alpha_{0}, \alpha_{1}) \nprec (\beta_{0}, \beta_{1})$, are said to be concurrent.
- _Critical Section_: A block of instructions that must be executed by only one thread. A critical section must not be concurrent.
- _Mutual Exclusion_: Property of having exclusive, non-concurrent, access to a resource.
- _Lock_ / _Unlock_: A lock, and unlock, mechanism is one that provides, or returns, a token indicating access to a resource. This can also be called _Acquire_ / _Release_.
- _Well Formed_: A thread is said to be well-formed if:
  - Each critical section is associated with a _unique_ lock/unlock mechanism.
  - The thread invokes the lock mechanism when trying to gain access to a critical section.
  - The thread calls the unlock mechanism when leaving a critical section.

Let $CS^{j}_{A}$ be the interval during which _A_ executes the critical section for the $j^{th}$ time. Then:
- **Mutual Exclusion**: Critical sections of different threads do not overlap. That is, for threads $A$ and $B$ and $j,k\in\mathbb{N}$ then:
```math
(CS^{k}_{A} \prec CS^{j}_{B}) \lor (CS^{j}_{B} \prec CS^{k}_{A})
```
- **Deadlock Freedom**: If some thread attempts to acquire a lock then some thread will succeed in acquiring the lock. That is, if thread $A$ attempts infinitely many times to enter a critical section protected by a lock but never succeeds then some other threads must be completing an infinite number of critical sections. 
- **Starvation Freedom**: Every thread that attempts to acquire a lock eventually succeeds.

If a thread has starvation freedom then it also has deadlock freedom.

We'll use $A(v \xrightarrow{W} x)$ to denote the event in which a thread $A$ assigns a value $v$ to some field $x$. Similiarly, we'll use $A(v \xleftarrow{R} x)$ to indicate that thread $A$ reads value $v$ from field $x$. Thought of another way, $A$ stores $v$ in field $x$ or loads $v$ from from $x$.

_Lemma_ 2.3.1: _LockOne_ algorithm satisfies mutual exclusion.

_Proof_: By way of contradiction, assume it does not have the mutual exclusion property. Then:

```math
\exists j,k\in\mathbb{N}: (CS^{j}_{A} \nprec CS^{k}_{B})\quad\&\quad(CS^{k}_{B} \nprec CS^{j}_{A})
```
Consider each thread's last execution of _lock_ prior to entering its $k^{th},j^{th}$ critical section. We see that:

```math
A(true\xrightarrow{W}flag[A]) \prec A(flag[B]\xleftarrow{R}false) \prec CS_{A}
```
```math
B(true\xrightarrow{W}flag[B]) \prec B(flag[A]\xleftarrow{R}false) \prec CS_{B}
```
```math
A(flag[B]\xleftarrow{R}false) \prec B(true\xrightarrow{W}flag[B])\\
```
Once $flag[B]$ is set to $true$ is remains $true$. It follow that the last relation holds since otherwise thread $A$ could not have read $flag[B]$ as $false$. The following follows from the transitivity of the precedence operator $\prec$.
```math
A(true\xrightarrow{W}flag[A]) \prec A(flag[B] \xleftarrow{R} false) \prec B(true\xrightarrow{W}flag[B]) \prec B(flag[A]\xleftarrow{R}false)
```
If follows that $A(true\xrightarrow{W}flag[A]) \prec B(flag[A]x\leftarrow{R}false)$ without an intervening write to the $flag[*]$, a contradiction. $_{\blacksquare}$. The lazy.

A starvation free lock is the _Peterson_ Lock:

```Java
class Peterson implements Lock {
  // thread-local index, 0 or 1
  private volatile boolean[] flag = new boolean[2];
  private volatile int victim;
  
  public void lock() {
    int i = ThreadID.get();
    int j = 1 - i;
    flag[i] = true;                    // I’m interested
    victim = i;                        // but you go first
    while (flag[j] && victim == i) {}; // I'll wait
  }
  
  public void unlock() {
    int i = ThreadID.get();
    flag[i] = false;                   // I’m not interested
  }
}
```

_Lemma_ 2.3.4: The Peterson Lock algorithm is starvation-free.

_Proof_: For a contradiction, suppose, without loss of generality, that thread $A$, with `ThreadID` of $0$, runs forever in the `lock()` method. It must be executing the `while` statement, waiting until either `flag[1]` becomes `false` or `victim` is set to $0$ (thread $A$).
What is $B$ doing while $A$ failed to make progress? Perhaps $B$ is repeatedly entering and leaving its critical section. Is so, then $B$ sets `victim` to $1$, its thread ID, as soon as it reenters the critical section. Once `victim` is set to $1$, it does not change and $A$ much eventually return from the `lock()` method-- a contradiction.

So it must be that $B$ is also stuck in its `lock()` method call, waiting until either `flag[0]` becomes `false` or `victim` is set to $0$. But `victim` cannot be both $0$ and $1$-- a contradiction. $_{\blacksquare}$

Some more definitions:
- _Doorway Section_: A section of instructions whose execution interval $D_{A}$ consists of a bounded number of time units.
- _Waiting Section_: A section of instructions whose execution interval $W_{A}$ may take an unbounded number of time units.
- _First-Come-First-Serve_: For threads $A,B$ and integers $j,k$, a lock is first-come-first-served if$A$ finished its doorway before thread $B$ starts its doorway. That is:
```math
D^{j}_{A} \prec D^{k}_{B} \implies CS^{j}_{A} \prec CS^{k}_{B}
```

Recall that a _cycle_ in a directed graph is a series nodes $n_{0}, \ldots, n_{k}$ such that there are directed edges from $n_{0}$ to $n_{1}$, from $n_{1}$ to $n_{2}$, and so on up node $n_{k-1}$ connected to $n_{k}$, and finally from $n_{k}$ to $n_{0}$.

For a graph G, a sub graph $A\in G$ _dominates_ $B\in G$ if every node in A has edges directed to every node in B. Let _graph multiplication_ be the non commutative composition $G \circ H$ such that:

Replace every node $v \in G$ by a copy of $H$, denoted by $H_{v}$, and let $H_{v}$ dominate $H_{u} \in (G \circ H)$ if $v$ dominates $u \in G$

Define a graph $T^{k}$ inductively to be: 
- $T^{1}$ is a single node.
- $T^{2}$ is the three-node graph defined earlier.
- For $k >2$, $T^{k} = T^{2} \circ T^{k-1}$.

A lock mechanism or object state $s$ is _inconsistent_ in any global state where some thread is in the critical section, but the lock state is compatible with a global state in which no thread is in the critical section or is trying to enter the critical section.

_Lemma_ 2.8.1: No deadlock-free `lock` algorithm can enter an inconsistent state.

_Proof_: Suppose the Lock object is in an inconsistent state $s$, where no thread is in the critical section or trying to enter. If thread $B$ tries to enter the critical section, it must block until $A$ leaves. We have a contradiction because $B$ cannot determine whether $A$ is in the critical section. $_{\blacksquare}$

A _covering state_ for a Lock object is one in which there is at least  one thread about to write to each shared location, but the Lock object’s locations “look” like the critical section is empty (i.e., the locations’ states appear as if there is no thread either in the critical section or trying to enter the critical section).

_Theorem_ 2.8.1: Any `Lock` algorithm that, by reading and writing memory, solves deadlock-free mutual exclusion for three threads, must use at least three distinct memory locations.

                            
### Discussion Board Posts

- Why do we need to use mutual exclusion locks? What are the benefits and issues for using mutual exclusion locks?
  - We need mutual exclusion locks to enforce mutual exclusion in critical section and thus prevent deadlocks and starvation and even enforce the validity of results for many algorithms. The book succinctly states this in the following way: "without this property we cannot guarantee that a computation's results are correct."
  - The benefit of a mutual exclusion lock is the ability to ensure that only one thread is able to acquire the lock or token at a time. A downside or issue of this is that mutual exclusion locks that operation on $n$ threads must use at least $n$ memory addresses (Theorem 2.8.1) 
    
- We used contradiction to proof that LockOne does provide mutual exclusions. Could you use the similar approach to proof that LockTwo provides mutual exclusion?
  - Yes you can. The book actually uses a proof-by-contradiction to prove this very thing (See Lemma 2.3.2). I suspect proof-by-contradiction could be used to prove mutual exclusion of a Lock<N> class though this may be untenable for even modest values of $N$.

- Could you use Peterson lock to write a short program to show that it works?
```Java
class PetersonDemo implements Runnable {

  public static Peterson petersonLock;

  public static class Peterson {
    // thread-local index, 0 or 1
    private volatile boolean[] flag = new boolean[2];
    private volatile int victim;
    
    public void lock(int myId) {
      int i = myId;
      int j = 1 - i;
      flag[i] = true;                    // I’m interested
      victim = i;                        // but you go first
      while (flag[j] && victim == i) {}; // wait here
    }
    
    public void unlock(int myId) {
      int i = myId;
      flag[i] = false; // I’m not interested
    }
  }

  public void criticalSection() {
    // gets the name of current thread
    System.out.println("Current Thread Name: " + Thread.currentThread().getName());
    
    // gets the ID of the current thread
    System.out.println("Current Thread ID: " + Thread.currentThread().getId());
  }
  
  // Thread and main functions  
  public void run() {
    // threadID mod 2 
    int myId = (int) (Thread.currentThread().getId() % 2);

    petersonLock.lock(myId);
    criticalSection();
    petersonLock.unlock(myId);
  }
  
  public static void main(String[] args) {
    // Create static lock object
    petersonLock = new Peterson();

    // Runnable target
    PetersonDemo t = new PetersonDemo();

    // create threads
    Thread t1 = new Thread(t, "First Thread");
    Thread t2 = new Thread(t, "Second Thread");

    // start threads
    t1.start();
    t2.start();
  }
}
```

The output of the above code is: 

```
Current Thread Name: First Thread
Current Thread ID: 9
Current Thread Name: Second Thread
Current Thread ID: 10
```

If we comment out the `petersonLock` lines then we get the following output:
```
Current Thread Name: First Thread
Current Thread Name: Second Thread
Current Thread ID: 9
Current Thread ID: 10
```

## Module Three

### Notes

Same as [previous section](#module-two)

### Discussion Board Posts

We have discussed about two-thread mutual exclusion locks and n-thread mutual exclusion locks. From your opinions, what are the major differences in construction locks in these two categories of locks?

A major difference is the amount of space required. As noted in Theorem 2.8.1, by way of a three-thread lock, that any lock that provides deadlock-freedom must use at least three distinct memory locations. Thus for a two-thread lock such as the Peterson Algorithm, it must use two memory locations while an n-thread lock will require n memory locations. For large amounts of threads this can quickly become unwieldy to think about. Additionally, there is the consideration for the overflow in Lamport's Bakery Algorithm. One could imagine, however unlikely, the filter algorithm also overflowing. On a 32 bit system, it would only be possible to have $2^{32}$ threads. Neither of these are considerations for the two-thread lock.


When we talk about mutual exclusion locks, we consider exclusiveness, starvation, and fairness as major criteria for the effectiveness of locks. Are there other features or properties that we need to consider?

I can think of a few. First, the complexity of the algorithm. If the algorithm is too complex it may be hard to extend, scale, or even implement. Building off of complexity, we may not just care about source code or implementation complexity but also runtime and space complexity. We can care about a starvation free lock but starvation freedom is still a loose criteria. It implies deadlock freedom but does not imply that any thread that wants to acquire a lock will do so in a _reasonable_ amount of time. That is, we may care that given n threads, what is the average time to acquire a lock. 


Can you proof that using fewer than N MRMW registers can not provide mutual exclusions for N threads?
  
_Proof by contradiction_:

Assume that we can provide mutual exclusion for $N$ threads without an inconsistent state. That is, without loss of generality, there is some algorithm that uses $N-1$ registers or memory locations. 

Consider a MRMW location such as the Peterson Algorithm's `victim` array. This array will have only $N-1$ entries. Initially, the state $s$ is such that no thread has attempted to enter the critical section. Since our lock provides deadlock-free mutual exclusion thread $N$ enters the critical section without writing to `victim`. When the remainder of threads acquire the lock they write to `victim` at a unique index and provide a complete covering of `victim`. One of these $N-1$ threads will acquire the lock and enter the critical section because state of the lock is not tracking that thread $N$ entered the critical section. This contradicts our assumption that we could provide mutual exclusion without an inconsistent state.

Suppose that instead $N-1$ threads provide a covering of the `victim` memory locations and some thread $t$ is allowed to enter the critical section. Now suppose that thread $N$ wants to enter the critical section. As such, it writes to some location in $j$ in the `victim` array that it wants access to the critical section. The thread that originally marked location $j$ will proceed to enter the critical section since `victim[j] = N`. Thus there will be two threads in the critical section. This contradicts our original assumption. 

Thus, there is no way to provide mutual exclusion for $N$ threads without $N$ registers.

### Homework

#### Problem 1

Use Amdahl’s law to answer the following questions:

Suppose the sequential part of a program accounts for 40% of the program’s execution time on a single processor. Find a limit for the overall speedup that can be achieved by running the program on a multiprocessor machine.

$$
\begin{align*}
S &= \lim_{n\to\infty}\frac{1}{(1 - p) + \frac{p}{n}}     \\
  &= \lim_{n\to\infty}\frac{1}{(1 - 0.6) + \frac{0.7}{n}} \\
  &= \lim_{n\to\infty}\frac{1}{0.4 + \frac{0.7}{n}}      \\
  &= \frac{1}{0.4} \\
  &= 2.5
\end{align*}
$$


Now suppose the sequential part accounts for 30% of the program’s computation time. Let $s_{n}$ be the program’s speedup on n processes, assuming the rest of the program is perfectly parallelizable. Your boss tells you to double this speedup: the revised program should have speedup $s'n > 2sn$. You advertise for a programmer to replace the sequential part with an improved version that runs k times faster. What value of k should you require?

I spent significantly more time on this problem than I'm sure was intended. But I've derived a general result which I'm pleased with.

Frist, recognize that for a parallizable portion $\beta$ and serial portion $\alpha$ we have $1 - \alpha = \beta$. Second, for any optimization factor $\Delta$ on $\alpha$ we have the optimized runtime $\alpha'=\frac{\alpha}{\Delta}$. Note that when the optimization factor $\Delta = 1 \implies \alpha' = \alpha$.  

$$
\begin{align*}
s'_{n} > \Delta s_{n} &\iff \frac{1}{\frac{alpha}{k} + \frac{1 - \frac{\alpha}{k}}{n}} > \Delta(\frac{1}{\alpha + \frac{1 - \alpha}{n}})\\
 &\iff \frac{1}{\frac{\alpha n + k - \alpha}{kn}} > \frac{\Delta}{\frac{\alpha n + 1 - \alpha}{n}} \\
 &\iff \frac{kn}{\alpha (n-1) + k} > \frac{n\Delta}{\alpha (n-1) + 1} \\
 &\iff kn(\alpha (n-1) + 1) > n\Delta(\alpha (n-1) + k) \\
 &\iff kn\alpha (n-1) + kn > n\Delta\alpha (n-1) + nk\Delta \\
 &\iff kn\alpha (n-1) + kn - nk\Delta > n\Delta\alpha (n-1) \\
 &\iff kn(\alpha (n-1) + 1 -\Delta) > n\Delta\alpha (n-1) \\
 &\iff k(\alpha (n-1) + 1 -\Delta) > \Delta\alpha (n-1) \\
 &\iff k > \frac{\Delta\alpha (n-1)}{(\alpha (n-1) + 1 -\Delta)} \\
 &\iff k > \{n-1}{n-1}\frac{\Delta\alpha}{(\alpha + \frac{1 -\Delta}{n-1})} \\
 &\iff k > \frac{\Delta\alpha}{\alpha + \frac{1 -\Delta}{n-1}}
\end{align*}
$$

In the problem statement we have $\Delta = 2$ and $\alpha = 0.3$. Thus the value of $k$ one should aim for is:

$$
\begin{align*}
k & > \frac{2 * 0.3}{0.3 + \frac{1 - 2}{n-1}} \\
  & > \frac{0.6}{0.3 - \frac{1}{n-1}}
\end{align*}
$$

Naturally, this is a function of $n$ as we have the parallel portion of the code to consider. But, in this problem we were asked to consider perfect parallelization. So that leads our final result:

$$
\begin{align*}
k > \lim_{n\to\infty} \frac{0.6}{0.3 - \frac{1}{n-1}} = 2
\end{align*}
$$

Thus, we want to make the serial portion of the program $k>2$ times faster.

Suppose the sequential part can be sped up three-fold, and when we do so, the modified program takes half the time of the original on n processors. What fraction of the overall execution time did the sequential part account for? Express your answer as a function of n.

We have $k = 3$ and $\Delta = 0.5$, then:

$$
\begin{align*}
\Delta s'_{n} &= \frac{1}{\frac{\alpha}{k} + \frac{1 - \alpha}{n}} \\
      &= \frac{1}{\frac{n\alpha}{nk} + \frac{k(1 - \alpha)}{kn}} \\
      &= \frac{1}{\frac{n\alpha + k(1 - \alpha)}{kn}} \\
      &= \frac{kn}{n\alpha + k(1 - \alpha)}
\end{align*}
$$

Now we solve for $\alpha$:

$$
\begin{align*}
\Delta s_{n} = \frac{kn}{n\alpha + k(1 - \alpha)} &\iff \Delta s'_{n}(n\alpha + k(1 - \alpha)) = kn \\
 &\iff \Delta s_{n}(n\alpha + k - k\alpha)) = kn \\
 &\iff \Delta s_{n}n\alpha + \Delta s_{n}k - \Delta s_{n}k\alpha = kn \\
 &\iff \alpha(n\Delta s_{n} - \Delta s_{n}k) + \Delta s_{n}k  = kn \\
 &\iff \alpha = \frac{kn - \Delta s_{n}k}{n\Delta s_{n} - \Delta s_{n}k}
\end{align*}
$$

If we assume that $s_{n} = 1 \implies \Delta s_{n} = 2s_{n} = 2$ for the program taking _half_ as long. Recall that we sped the serial portion up by $k = 3$ fold. Thus:

$$
\begin{align*}
\alpha(n) &= \frac{kn - \Delta s_{n}k}{n\Delta s_{n} - \Delta s_{n}k} \\
        &= \frac{3n - (2)(3)}{2n - (2)(3)}\\
        &= \frac{3(n - 2)}{2(n - 3)}\\
        &= \frac{3}{2}\frac{n - 2}{n - 3}\\
\end{align*}
$$

#### Problem 2

You have a choice between buying one uniprocessor that executes five zillion instructions per second, or a ten-processor multiprocessor where each processor executes one zillion instructions per second. Using Amdahl’s Law, explain how you would decide which to buy for a particular application.

#### Problem 3:

Programmers at the Flaky Computer Corporation designed the protocol shown blow to achieve n-thread mutual exclusion. For each question, either sketch a proof, or display an execution where it fails.

Does this protocol satisfy mutual exclusion?
Is this protocol starvation-free?
Is this protocol deadlock-free?

The Flaky lock

```Java
class Flaky implements Lock {

   private int turn;
   private boolean busy = false;

   public void lock() {
      int me = ThreadID.get();

      do {
          do {
                  turn = me;
           } while (busy);
          busy = true;
      } while (turn != me);
   }

   public void unlock() {
       busy = false;
   }
}
```

#### Problem 4

In practice, almost all lock acquisitions are uncontended, so the most practical measure of a lock’s performance is the number of steps needed for a thread to acquire a lock when no other thread is concurrently trying to acquire the lock.

Scientists at Cantaloupe-Melon University have devised the following “wrapper” for an arbitrary lock, shown below. They claim that if the base Lock class provides mutual exclusion and is starvation-free, so does the FastPath lock, but it can be acquired in a constant number of steps in the absence of contention. Sketch an argument why they are right, or give a counterexample.

```Java
class FastPath implements Lock {

 private static ThreadLocal<Integer> myIndex;
 private Lock lock;
 private int x, y = -1;

 public void lock() {
     int i = myIndex.get();
     x = i;                             // I’m here
     while (y != -1) { }          // is the lock free?
      y = i;                            // me again?
     if (x != i)                      // Am I still here?
        lock.lock();               // slow path
  }
  
  public void unlock() {
     y = -1;
     lock.unlock();
  }
}
```

These exercises must be completed and submitted no later than Day 7 of Module 5.

## Module Four

### Notes

Reading:
  Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
  Chapter 3: Concurrent Objects

_Principle 3.3.1_: Method calls should appear to happen in a one-at-a-time sequential order.

Definition: An object is _quiescent_ if it has no pending method calls.

_Principle 3.3.2_: Method calls separated by a period of quiescence should appear to take affect in their "real-time" order.

Together Principles 3.3.1 & 3.3.2 define a correctness property called _quiescent consistency_. This is a _nonblocking_ correctness condition as well.

A correctness property $\mathbb{P}$ is _compositional_ $\iff \forall \theta \in \mathbb{S}, \theta \in \mathbb{P}$. That is to say, if every object $\theta$ in a system $\mathbb{S}$ has property $\mathbb{P}$ then $\mathbb{S}$ has $\mathbb{P}$.

A method on an object is _total_ if it is well-defined for every state of the object. Otherwise it is _partial_. For example pushing on to an infinite stack is a _total_ method. However, popping is _partial_ because one can not pop an object from an empty stack.

_Principle 3.4.1_: Method calls should appear to take effect in program order.

Principle 3.4.1 together with Principles 3.3.1 & 3.3.2 define the correctnes property called _sequential consistency_, which is widely used in the literature on multiprocessor synchronization.

Note that quiescent consistency and sequential consistency are _incomparable_. That is, it is possible to have one property without the other.

The concepts of _barrier_ and _fence_ are used to enforce sequential consistence between two regions. By that I mean that if we have two add instruction, a barrier, followed by two subtraction instructions then we can be sure that the two subtraction instructions will follow the two addition instructions. We cannot, however, be sure of the order of the two addition instruction prior to the barrier. Similarly, we cannot be sure of the ordering of the two subtraction instructions following the barrier.

Sequential consistency is not compositional.

_Principle 3.5.1_: Each method call should appear to take effect instantaneously at the some moment between its invocation and its response.

The above principle states that the "real-time" behavior of method calls must be preserved. This correctness property is called _linearizability_. linearizability is a stronger property than sequential consistency thus every linerizable execution is sequentially consistent but not every sequentially consistent execution is linearizable. The usual way to show that a concurrent object implementation is linearizable is to identify for each method a _linearization point_ where the method takes effect. For lock-based implementations, each method's critical section can serve as its linearization point. For implementations which do not make use of locks, the linearization point is typically a single step at which point the side effects of the method call are visible or available to other method calls. The property linearizablity is compositional.  

The execution of a concurrent system is modeled by by a _history_, a finite sequence of method invocation and response events. A _subhistory_ of a history $\mathbb{H}$ is a subset of sequences in $\mathbb{H}$. We write the invocation of a method as $\langle x.m(a*) A \rangle$ for method $m$ on object $x$ in thread $A$ with arguments $a*$. Similarly, a response event is $\langle x:t(r*) A \rangle$ where $t$ is either $OK$ or an exception name and $r*$ is a sequence of result values.

A _method call_ is a pair consisting of an invocation and its matching response both in a history $\mathbb{H}$. An invocation is _pending_ in $\mathbb{H}$ if no matching response following the invocation. An _extension_ of a history $\mathbb{H}$ by history $\mathbb{H}'$ is $\mathbb{H}$ appended with $\mathbb{H}'$. A history is said to be complete if it contains no pending invocations.

A  _subhistory_ $\mathbb{H} | C$,  ($\mathbb{H}$ at $C$), for history $\mathbb{H}$ and some class of events $C$, is a subsequence of all events in $\mathbb{H}$  whose thread context is compromised of the object class $C$. The object class can be a thread or object. Two history are equivalent if and only if $\mathbb{H} | A = \mathbb{H}' | A, \forall A \in \mathbb{H}$. A history is _well-formed_ if every thread sub-history in the history is sequential.

A _sequential specification_ for an object is a set sequential histories for the object. A sequential history is _legal_ if each object subhistory is legal for that object. This definition given by the book is recursive and I don't think it makes much sense. 

Recall that a partial order $\rightarrow$ is irreflexive and transitive. That is some event $x$ cannot precede itself (irreflexive). A _total order_ $<$ on $\mathbb{X}$ is a partial order such that $\forall x,y\in\mathbb{X}, x != y V (x < y)$.

_Fact 3.6.1_: If $\rightarrow$ is a partial order on $\mathbb{X}$, then there exists a relation $<$ on $\mathbb{X}$ such that $x,y\in\mathbb{X}: x \rightarrow y \implies x < y$.

For methods $m_{0}, m_{1}$ and history $\mathbb{H}$, we say that $m_{0} \rightarrow_{\mathbb{H}} m_{1}$ if $m_{0}$ precedes $m_{1}$ in $\mathbb{H}$.


### Discussion Board Posts

In this module, we talked about linearizability and sequential consistency. Both of them belongs to concurrent specifications. What are the purposes of concurrent specifications? Why do we need them?


In the linearizability analysis, we always exam the sequence of thread executions over concurrent objects and decide if it can be considered as linearizability. Why do we care of this? Why linearizability is important?

_Definition 3.6.1_: A history $\mathbb{H}$ is linearizable if it has an extention $\mathbb{H}'$ and there is a legal sequential history $\mathbb{S}$ such that:
- complete($\mathbb{S}) = $\mathbb{S}$
- if $m_{0} \rightarrow_{\mathbb{H}} m_{1} \implies  m_{0} \rightarrow_{\mathbb{S}} m_{1}$

Note that a history may have many linearizations.


In our lecture, there is an example that a history of concurrent objects can be considered as sequential consistency but not linearizability. What are the meanings of that? Can we consider another history that is linearizability but not sequential consistency, and explain your conclusions.
