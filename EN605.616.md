# EN.605.616.81.SP23: Multiprocess Architecture & Programming

## Module One

### Notes:

Reading: 
  Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
  Chapter 1: Introduction
  
_Mutual Exclusion_ is the problem or property that only one entity is allowed in a critical section. I.e. access is mutually exclusive. Necessary properties of mutual exclusion are _deadlock-free_ and _lockout-freedom_. Deadlock-free is defined as an entity that wants to enter a critical section eventually succeeds. If two entities want to enter the critical section then one of them eventually succeeds. Starvation-freedom is the property that if an entity wants to enter the critical section then it will eventually do so. The final property that is useful for mutual exclusion is _waiting_. Waiting is that if two entities desire access to the critical section and one of them get access, the other will wait until the one with access finishes.

The _producer-consumer_ problem is when there are two entities. One entity supplies some resource and another consumes the resource. The problem is coordinating the interaction between the two entities such that the consumer only accesses the resources when the producer has made them available. Additionally, the producing entity will not produce more resource than can be consumed. Notes that mutual exclusion cannot be used for the producer-consumer module as the consumer needs to be free to "check" for the resource as often as desired. 

_Amdahl's Law_ captures the idea of how much speed up we can get through single-level parallelization. The maxiumum speedup $S$ that can be acchived by $n$ processors collaborating on some application where $p$ is the fraction of the application that is parallelizable. Amdahl's Law states that the speed up by adding a parallel portion $\frac{p}{n}$ is:

$$
S = \frac{1}{1 - p + \frac{p}{n}}
$$

Note that this does not account for different levels of values of parallelization. There is strictly a single sequential portion and single parallelizable prortion. For an extention to Amdahl's Law that includes multi-level parallelism (i.e. processes and threads) read [Speedup for Multi-level Parallel Computing](https://www.comp.nus.edu.sg/~hebs/pub/shangjianghips12.pdf).

From the book we have the following excerpt on Amdahl's Law:
```
In general, however, for a given problem and a ten-processor machine,
Amdahl’s Law says that even if we manage to parallelize 90% of the solution,
but not the remaining 10%, then we end up with a five-fold speedup, but not
a ten-fold speedup. In other words, the remaining 10% that we did not paral-
lelize cut our utilization of the machine in half. 
```

  
### Discussion Board Posts

- What are the differences for the requirements between mutual exclusion mechanisms and producer/consumer mechanisms in parallel computing?
  - Mutual exclusion is controlled access to some critical section. Mutual exclusion is not necessarily about resources. However, the producer-consumer patter is about exposing resources. In the mutual exclusion pattern we require that if there are some entities that desire access to a resource or critical section then at least one of them succeeds. Note that this means there may be $n-1$ entities that never get access the resource or critical section. That is they "starve". However, in the producer-consumer pattern we have the requirement that any entity that wants to access the resource or critical section then it will succeed. That is if there are $n$ entities that require access to a resource or critical secton then $n$ entites will eventually be granted access.
  - The most obvious difference is that producer-consumer has the more strict requirement of starvation-free.

- Can we use Amdahl’s law more generically? For example, there is a software that will be executed in a 10-processor computer. The software has 20% of it can only be run in sequential, 30% of it can be run concurrently in 5 processors, and the remaining 50% of it can be run concurrently in all 10 processors. What is the speedup for this execution comparing to a sequential execution?
  - I was unable to find an extension or generalization of _Amdahl's Law_ that includes different percentages of parallelization across processors as described in the question. However, I was able to find a paper ([Speedup for Multi-Level Parallel Computing](https://www.comp.nus.edu.sg/~hebs/pub/shangjianghips12.pdf) which described an extension to Amdahl's Law that map much better to my experience in parallel computing. That is, it describes an extension to Amdahl's Law that accounts for multi-level parallelism afforded in modern computing environments. Specifically, the mutli-level here refers to process level parallelism combined with thread level parallelism. In distributed computing you may have an application that runs the same process, with shared memory, over some number of nodes. Assume for simplicity that there is one process per node. Then we have processes $p$ equals the number of nodes $n$. For each process we have another level of parallelism at the thread level. That is for each process $p_{i}$, we have $t_{i}$ threads for parallelism at the second level. More accurately, the thread-level parallelism is concurrency and is defined by $\alpha : 0 \leq \alpha \leq 1$. The multi-process portion is parallelization $\beta : 0 \leq \beta \leq 1$. The we have the multi-level speedup as:
    - $sp(\alpha,\beta,p,t) = sp(i = 1) = \frac{1}{1 - \alpha + \frac{\alpha(1 - \beta + \frac{\beta}{t})}{p}}$

- The asynchrony of processor (or thread) executions is bad for parallel computing. Why is that? Explain your opinions.
  - The asynvhony of a process or thread execution is bad because it conflict with our intuitions of how source code and the resulting assembly ought to run. Specifically, this affects our intitions of around [sequential consistency](https://www.educative.io/answers/what-is-sequential-consistency-in-distributed-systems). For example, in the case of threads, we may not have insight in to how the operating system will schedule the programs threads. In fact we should program in such a way that we do not rely on thread execution ording. At the processor, the CPU may [reorder memory operations](https://en.wikipedia.org/wiki/Memory_ordering). Such reordering can be, and is done, by the compiler in languages such as C. At the network level, we may do some network transaction and on large systems there may be no guaranteed delivery ordering. 
  - The discrepancy between how we think about ordering and the systems we depend on to compile and execute our programs lead to inefficiencies, bugs, and vulnerabilities. 
  - There are tools to enforce ordering in each of these contexts. In the thread and processer contexts we can use serialized instructions [serialize](https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-isa-instructions/serialize/serialize-1.html) and [atomic instructions](https://www.ibm.com/docs/en/aix/7.2?topic=services-atomic-operations). In shared memory and networking context we can use function calls such as _quiet_ and _fence_ which enforce different constraints on delivery of network traffic [see: SHMEM 1.5](http://www.openshmem.org/site/sites/default/site_files/openshmem-1.5rc1.pdf). 
    
## Module Two

### Notes 
Reading: 
  Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
  Chapter 1: Introduction

Definitions:
- _Instantaneous_: An event is instaneous if it occurs within a single time unit. 
- _Preceeds_: an event $\alpha$ preceeds another event $\beta$, denoted $\alpha \prec \beta$, if $\alpha$ occurs at a time prior to $\beta$.
- _Interval_: an interval a $(\alpha_{0}, \alpha_{1})$ is the number of time units between event $\alpha_{0}$ and $\alpha_{1}$.
- _Concurrent_: Intervals $(\alpha_{0}, \alpha_{1})$ and $(\beta_{0}, \beta_{1})$ that do not have the preceeds relation, $(\alpha_{0}, \alpha_{1}) \nprec (\beta_{0}, \beta_{1})$, are said to be concurrent.
- _Critical Section_: A block of instructions that must be executed by only one thread. A critical section must not be concurrent.
- _Mutual Exclusion_: Property of having exclusive, non-concurrent, access to a resource.
- _Lock_ / _Unlock_: A lock, and unlock, mechanism is one that provides, or returns, a token indicating access to a resource. This can also be called _Acquire_ / _Release_.
- _Well Formed_: A thread is said to be well-formed if:
  - Each critical section is associated with a _unique_ lock/unlock mechanism.
  - The thread invokes the lock mechanism when trying to gain access to a critical section.
  - The thread calls the unlock mechanism when leaving the a critical section.

Let $CS^{j}_{A}$ be the interval during which _A_ executes the critical section for the $j^{th}$ time. Then:
- **Mutual Exclusion**: Critical sections of different threads do not overlap. That is, for threads $A$ and $B$ and $j,k\in\mathbb{N}$ then either $CS^{k}_{A} \prec CS^{j}_{B}$ or $CS^{j}_{B} \prec CS^{k}_{A}$.
- **Deadlock Freedom**: If some thread attempts to acquire a lock then some thread will succeed in acquiring the lock. That is, if thread $A$ attempts infinitely many times to enter a critical section protected by a lock but never succeeds then some other threads must be completing an infinite number of critical sections. 
- ** Starvation Freedom**: Every thread that attempts to acquire a lock eventually succeeds.

If a thread has starvation freedom then it also has deadlock freedom.

We'll use $A(v \xrightarrow{W} x)$ to denote the event in which a thread $A$ assigns a value $v$ to some field $x$. Similiarly, we'll use $A(v \xleftarrow{R} x)$ to indicate that thread $A$ reads value $v$ from field $x$. Thought of another way, $A$ stores $v$ in field $x$ or loads $v$ from from $x$.

_Lemma_ 2.3.1: _LockOne_ algorithm satisfies mutual exclusion.

_Proof_: By way of contradition, assume it does not have the mutual exclusion property. Then:

```math
\exists j,k\in\mathbb{N}: (CS^{j}_{A} \nprec CS^{k}_{B})\quad\&\quad(CS^{k}_{B} \nprec CS^{j}_{A})
```
Consider each thread's last execution of _lock_ prior to entering its $k^{th},j^{th}$ critical section. We see that:

```math
A(true\xrightarrow{W}flag[A]) \prec A(flag[B]\xleftarrow{R}false) \prec CS_{A}
```
```math
B(true\xrightarrow{W}flag[B]) \prec B(flag[A]\xleftarrow{R}false) \prec CS_{B}
```
```math
A(flag[B]\xleftarrow{R}false) \prec B(true\xrightarrow{W}flag[B])\\
```
Once $flag[B]$ is set to $true$ is remains $true$. It follow that the last relation holds since otherwise thread $A$ could not have read $flag[B]$ as $false$. The following follows from the transitivity of the precedence operator $\prec$.
```math
A(true\xrightarrow{W}flag[A]) \prec A(flag[B] \xleftarrow{R} false) \prec B(true\xrightarrow{W}flag[B]) \prec B(flag[A]\xleftarrow{R}false)
```
If follows that $A(true\xrightarrow{W}flag[A]) \prec B(flag[A]x\leftarrow{R}false)$ without an invervening write to the $flag[*]$, a contradiction. $_{\blacksquare}$

A starvation free lock is the _Peterson_ Lock:

```Java
class Peterson implements Lock {
  // thread-local index, 0 or 1
  private volatile boolean[] flag = new boolean[2];
  private volatile int victim;
  
  public void lock() {
    int i = ThreadID.get();
    int j = 1 - i;
    flag[i] = true;                    // I’m interested
    victim = i;                        // but you go first
    while (flag[j] && victim == i) {}; // I'll wait
  }
  
  public void unlock() {
    int i = ThreadID.get();
    flag[i] = false;                   // I’m not interested
  }
}
```

_Lemma_ 2.3.4: The Peterson Lock algorithm is starvation-free.

_Proof_: For a contradiction, suppose, without loss of generality, that thread $A$, with `ThreadID` of $0$, funs forever in the `lock()` method. It must be executing the `while` statement, waiting until either `flag[1]` becomes `false` or `victim` is set to $0$ (thread $A$).
What is $B$ doing while $A$ failed to make progress? Perhaps $B$ is repeatedly entering and leaving its critical section. Is so, then $B$ sets `victim` to $1$, its thread ID, as soon as it reenters the critical section. Once `victim` is set to $1$, it does not change and $A$ much eventually return from the `lock()` method-- a contradiction.

So it must be that $B$ is also stuck in its `lock()` method call, waiting until either `flag[0]` becomes `false` or `victim` is set to $0$. But `victim` cannot be both $0$ and $1$-- a contradiction. $_{\blacksquare}$

Some more definitions:
- _Doorway Section_: A section of instructions whose execution interval $D_{A}$ consists of a bounded number of time units.
- _Waiting Section_: A section of instructions whose execution interval $W_{A}$ may take an unbounded number of time units.
- _First-Come-First-Serve_: For threads $A,B$ and integers $j,k$, a lock is first-come-first-served if$A$ finished its doorway before thread $B$ starts its doorway. That is:
```math
D^{j}_{A} \prec D^{k}_{B} \implies CS^{j}_{A} \prec CS^{k}_{B}
```

Recall that a _cycle_ in a directed graph is a series nodes $n_{0}, \ldots, n_{k}$ such that ther are directed edges from $n_{0}$ to $n_{1}$, from $n_{1}$ to $n_{2}$, and so on up node $n_{k-1}$ connected to $n_{k}$, and finally from $n_{k}$ to $n_{0}$.

For a graph G, a subgraph $A\in G$ _dominates_ $B\in G$ if every node in A has edges directed to every node in B. Let _graph multiplication_ be the non-communitive composition $G \circ H$ such that:

Replace every node $v \in G$ by a copy of $H$, denoted by $H_{v}$, and let $H_{v}$ dominate $H_{u} \in (G \circ H)$ if $v$ dominates $u \in G$

Define a graph $T^{k}$ inductively to be: 
- $T^{1}$ is a single node.
- $T^{2}$ is the three-node graph defined earlier.
- For $k >2$, $T^{k} = T^{2} \circ T^{k-1}$.

A lock mechanism or object state $s$ is _inconsistent_ in any global state where some thread is in the critical section, but the lock state is compatible with a global state in which no thread is in the critical section or is trying to enter the critical section.

_Lemma_ 2.8.1: No deadlock-free `lock` algorithm can enter an inconsistent state.

_Proof_: Suppose the Lock object is in an inconsistent state $s$, where no thread is in the critical section or trying to enter. If thread $B$ tries to enter the critical section, it must block until $A$ leaves. We have a contradiction because $B$ cannot determine whether $A$ is in the critical section. $_{\blacksquare}$

A _covering state_ for a Lock object is one in which there is at least  one thread about to write to each shared location, but the Lock object’s locations “look” like the critical section is empty (i.e., the locations’ states appear as if there is no thread either in the critical section or trying to enter the critical section).

_Theorem_ 2.8.1: Any `Lock` algorithm that, by reading and writing memory, solves deadlock-free mutual exclusion for three threads, must use at least three distinct memory locations.

  
### Discussion Board Posts
