---
title: EN.605.616.81.SP23
description: The JHU course Multiprocess Architecture & Programming.
---

# EN.605.616.81.SP23: Multiprocess Architecture & Programming

## Module One

### Notes:

Reading: 
  Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
  Chapter 1: Introduction
  
_Mutual Exclusion_ is the problem or property that only one entity is allowed in a critical section. I.e. access is mutually exclusive. Necessary properties of mutual exclusion are _deadlock-free_ and _lockout-freedom_. Deadlock-free is defined as an entity that wants to enter a critical section eventually succeeds. If two entities want to enter the critical section then one of them eventually succeeds. Starvation-freedom is the property that if an entity wants to enter the critical section then it will eventually do so. The final property that is useful for mutual exclusion is _waiting_. Waiting is that if two entities desire access to the critical section and one of them get access, the other will wait until the one with access finishes.

The _producer-consumer_ problem is when there are two entities. One entity supplies some resource and another consumes the resource. The problem is coordinating the interaction between the two entities such that the consumer only accesses the resources when the producer has made them available. Additionally, the producing entity will not produce more resource than can be consumed. Notes that mutual exclusion cannot be used for the producer-consumer module as the consumer needs to be free to "check" for the resource as often as desired. 

_Amdahl's Law_ captures the idea of how much speed up we can get through single-level parallelization. The maxiumum speedup $S$ that can be acchived by $n$ processors collaborating on some application where $p$ is the fraction of the application that is parallelizable. Amdahl's Law states that the speed up by adding a parallel portion $\frac{p}{n}$ is:

$$
S = \frac{1}{1 - p + \frac{p}{n}}
$$

Note that this does not account for different levels of values of parallelization. There is strictly a single sequential portion and single parallelizable prortion. For an extention to Amdahl's Law that includes multi-level parallelism (i.e. processes and threads) read [Speedup for Multi-level Parallel Computing](https://www.comp.nus.edu.sg/~hebs/pub/shangjianghips12.pdf).

From the book we have the following excerpt on Amdahl's Law:
```
In general, however, for a given problem and a ten-processor machine,
Amdahl’s Law says that even if we manage to parallelize 90% of the solution,
but not the remaining 10%, then we end up with a five-fold speedup, but not
a ten-fold speedup. In other words, the remaining 10% that we did not paral-
lelize cut our utilization of the machine in half. 
```

  
### Discussion Board Posts

- What are the differences for the requirements between mutual exclusion mechanisms and producer/consumer mechanisms in parallel computing?
  - Mutual exclusion is controlled access to some critical section. Mutual exclusion is not necessarily about resources. However, the producer-consumer patter is about exposing resources. In the mutual exclusion pattern we require that if there are some entities that desire access to a resource or critical section then at least one of them succeeds. Note that this means there may be $n-1$ entities that never get access the resource or critical section. That is they "starve". However, in the producer-consumer pattern we have the requirement that any entity that wants to access the resource or critical section then it will succeed. That is if there are $n$ entities that require access to a resource or critical secton then $n$ entites will eventually be granted access.
  - The most obvious difference is that producer-consumer has the more strict requirement of starvation-free.

- Can we use Amdahl’s law more generically? For example, there is a software that will be executed in a 10-processor computer. The software has 20% of it can only be run in sequential, 30% of it can be run concurrently in 5 processors, and the remaining 50% of it can be run concurrently in all 10 processors. What is the speedup for this execution comparing to a sequential execution?
  - I was unable to find an extension or generalization of _Amdahl's Law_ that includes different percentages of parallelization across processors as described in the question. However, I was able to find a paper ([Speedup for Multi-Level Parallel Computing](https://www.comp.nus.edu.sg/~hebs/pub/shangjianghips12.pdf) which described an extension to Amdahl's Law that map much better to my experience in parallel computing. That is, it describes an extension to Amdahl's Law that accounts for multi-level parallelism afforded in modern computing environments. Specifically, the mutli-level here refers to process level parallelism combined with thread level parallelism. In distributed computing you may have an application that runs the same process, with shared memory, over some number of nodes. Assume for simplicity that there is one process per node. Then we have processes $p$ equals the number of nodes $n$. For each process we have another level of parallelism at the thread level. That is for each process $p_{i}$, we have $t_{i}$ threads for parallelism at the second level. More accurately, the thread-level parallelism is concurrency and is defined by $\alpha : 0 \leq \alpha \leq 1$. The multi-process portion is parallelization $\beta : 0 \leq \beta \leq 1$. The we have the multi-level speedup as:
```math
sp(\alpha,\beta,p,t) = sp(i = 1) = \frac{1}{1 - \alpha + \frac{\alpha(1 - \beta + \frac{\beta}{t})}{p}}
```

- The asynchrony of processor (or thread) executions is bad for parallel computing. Why is that? Explain your opinions.
  - The asynvhony of a process or thread execution is bad because it conflict with our intuitions of how source code and the resulting assembly ought to run. Specifically, this affects our intitions of around [sequential consistency](https://www.educative.io/answers/what-is-sequential-consistency-in-distributed-systems). For example, in the case of threads, we may not have insight in to how the operating system will schedule the programs threads. In fact we should program in such a way that we do not rely on thread execution ording. At the processor, the CPU may [reorder memory operations](https://en.wikipedia.org/wiki/Memory_ordering). Such reordering can be, and is done, by the compiler in languages such as C. At the network level, we may do some network transaction and on large systems there may be no guaranteed delivery ordering. 
  - The discrepancy between how we think about ordering and the systems we depend on to compile and execute our programs lead to inefficiencies, bugs, and vulnerabilities. 
  - There are tools to enforce ordering in each of these contexts. In the thread and processer contexts we can use serialized instructions [serialize](https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-isa-instructions/serialize/serialize-1.html) and [atomic instructions](https://www.ibm.com/docs/en/aix/7.2?topic=services-atomic-operations). In shared memory and networking context we can use function calls such as _quiet_ and _fence_ which enforce different constraints on delivery of network traffic [see: SHMEM 1.5](http://www.openshmem.org/site/sites/default/site_files/openshmem-1.5rc1.pdf). 
    
## Module Two

### Notes 
Reading: 
  Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
  Chapter 2

Definitions:
- _Instantaneous_: An event is instantaneous if it occurs within a single time unit. 
- _Preceeds_: an event $\alpha$ preceeds another event $\beta$, denoted $\alpha \prec \beta$, if $\alpha$ occurs at a time prior to $\beta$.
- _Interval_: an interval a $(\alpha_{0}, \alpha_{1})$ is the number of time units between event $\alpha_{0}$ and $\alpha_{1}$.
- _Concurrent_: Intervals $(\alpha_{0}, \alpha_{1})$ and $(\beta_{0}, \beta_{1})$ that do not have the preceeds relation, $(\alpha_{0}, \alpha_{1}) \nprec (\beta_{0}, \beta_{1})$, are said to be concurrent.
- _Critical Section_: A block of instructions that must be executed by only one thread. A critical section must not be concurrent.
- _Mutual Exclusion_: Property of having exclusive, non-concurrent, access to a resource.
- _Lock_ / _Unlock_: A lock, and unlock, mechanism is one that provides, or returns, a token indicating access to a resource. This can also be called _Acquire_ / _Release_.
- _Well Formed_: A thread is said to be well-formed if:
  - Each critical section is associated with a _unique_ lock/unlock mechanism.
  - The thread invokes the lock mechanism when trying to gain access to a critical section.
  - The thread calls the unlock mechanism when leaving a critical section.

Let $CS^{j}_{A}$ be the interval during which _A_ executes the critical section for the $j^{th}$ time. Then:
- **Mutual Exclusion**: Critical sections of different threads do not overlap. That is, for threads $A$ and $B$ and $j,k\in\mathcal{N}$ then:
```math
(CS^{k}_{A} \prec CS^{j}_{B}) \lor (CS^{j}_{B} \prec CS^{k}_{A})
```
- **Deadlock Freedom**: If some thread attempts to acquire a lock then some thread will succeed in acquiring the lock. That is, if thread $A$ attempts infinitely many times to enter a critical section protected by a lock but never succeeds then some other threads must be completing an infinite number of critical sections. 
- **Starvation Freedom**: Every thread that attempts to acquire a lock eventually succeeds.

If a thread has starvation freedom then it also has deadlock freedom.

We'll use $A(v \xrightarrow{W} x)$ to denote the event in which a thread $A$ assigns a value $v$ to some field $x$. Similiarly, we'll use $A(v \xleftarrow{R} x)$ to indicate that thread $A$ reads value $v$ from field $x$. Thought of another way, $A$ stores $v$ in field $x$ or loads $v$ from from $x$.

_Lemma_ 2.3.1: _LockOne_ algorithm satisfies mutual exclusion.

_Proof_: By way of contradiction, assume it does not have the mutual exclusion property. Then:

```math
\exists j,k\in\mathcal{N}: (CS^{j}_{A} \nprec CS^{k}_{B})\quad\&\quad(CS^{k}_{B} \nprec CS^{j}_{A})
```
Consider each thread's last execution of _lock_ prior to entering its $k^{th},j^{th}$ critical section. We see that:

```math
A(true\xrightarrow{W}flag[A]) \prec A(flag[B]\xleftarrow{R}false) \prec CS_{A}
```
```math
B(true\xrightarrow{W}flag[B]) \prec B(flag[A]\xleftarrow{R}false) \prec CS_{B}
```
```math
A(flag[B]\xleftarrow{R}false) \prec B(true\xrightarrow{W}flag[B])\\
```
Once $flag[B]$ is set to $true$ is remains $true$. It follow that the last relation holds since otherwise thread $A$ could not have read $flag[B]$ as $false$. The following follows from the transitivity of the precedence operator $\prec$.
```math
A(true\xrightarrow{W}flag[A]) \prec A(flag[B] \xleftarrow{R} false) \prec B(true\xrightarrow{W}flag[B]) \prec B(flag[A]\xleftarrow{R}false)
```
If follows that $A(true\xrightarrow{W}flag[A]) \prec B(flag[A]x\leftarrow{R}false)$ without an intervening write to the $flag[*]$, a contradiction. $_{\blacksquare}$. The lazy.

A starvation free lock is the _Peterson_ Lock:

```Java
class Peterson implements Lock {
  // thread-local index, 0 or 1
  private volatile boolean[] flag = new boolean[2];
  private volatile int victim;
  
  public void lock() {
    int i = ThreadID.get();
    int j = 1 - i;
    flag[i] = true;                    // I’m interested
    victim = i;                        // but you go first
    while (flag[j] && victim == i) {}; // I'll wait
  }
  
  public void unlock() {
    int i = ThreadID.get();
    flag[i] = false;                   // I’m not interested
  }
}
```

_Lemma_ 2.3.4: The Peterson Lock algorithm is starvation-free.

_Proof_: For a contradiction, suppose, without loss of generality, that thread $A$, with `ThreadID` of $0$, runs forever in the `lock()` method. It must be executing the `while` statement, waiting until either `flag[1]` becomes `false` or `victim` is set to $0$ (thread $A$).
What is $B$ doing while $A$ failed to make progress? Perhaps $B$ is repeatedly entering and leaving its critical section. Is so, then $B$ sets `victim` to $1$, its thread ID, as soon as it reenters the critical section. Once `victim` is set to $1$, it does not change and $A$ much eventually return from the `lock()` method-- a contradiction.

So it must be that $B$ is also stuck in its `lock()` method call, waiting until either `flag[0]` becomes `false` or `victim` is set to $0$. But `victim` cannot be both $0$ and $1$-- a contradiction. $_{\blacksquare}$

Some more definitions:
- _Doorway Section_: A section of instructions whose execution interval $D_{A}$ consists of a bounded number of time units.
- _Waiting Section_: A section of instructions whose execution interval $W_{A}$ may take an unbounded number of time units.
- _First-Come-First-Serve_: For threads $A,B$ and integers $j,k$, a lock is first-come-first-served if$A$ finished its doorway before thread $B$ starts its doorway. That is:
```math
D^{j}_{A} \prec D^{k}_{B} \implies CS^{j}_{A} \prec CS^{k}_{B}
```

Recall that a _cycle_ in a directed graph is a series nodes $n_{0}, \ldots, n_{k}$ such that there are directed edges from $n_{0}$ to $n_{1}$, from $n_{1}$ to $n_{2}$, and so on up node $n_{k-1}$ connected to $n_{k}$, and finally from $n_{k}$ to $n_{0}$.

For a graph G, a sub graph $A\in G$ _dominates_ $B\in G$ if every node in A has edges directed to every node in B. Let _graph multiplication_ be the non commutative composition $G \circ H$ such that:

Replace every node $v \in G$ by a copy of $H$, denoted by $H_{v}$, and let $H_{v}$ dominate $H_{u} \in (G \circ H)$ if $v$ dominates $u \in G$

Define a graph $T^{k}$ inductively to be: 
- $T^{1}$ is a single node.
- $T^{2}$ is the three-node graph defined earlier.
- For $k >2$, $T^{k} = T^{2} \circ T^{k-1}$.

A lock mechanism or object state $s$ is _inconsistent_ in any global state where some thread is in the critical section, but the lock state is compatible with a global state in which no thread is in the critical section or is trying to enter the critical section.

_Lemma_ 2.8.1: No deadlock-free `lock` algorithm can enter an inconsistent state.

_Proof_: Suppose the Lock object is in an inconsistent state $s$, where no thread is in the critical section or trying to enter. If thread $B$ tries to enter the critical section, it must block until $A$ leaves. We have a contradiction because $B$ cannot determine whether $A$ is in the critical section. $_{\blacksquare}$

A _covering state_ for a Lock object is one in which there is at least  one thread about to write to each shared location, but the Lock object’s locations “look” like the critical section is empty (i.e., the locations’ states appear as if there is no thread either in the critical section or trying to enter the critical section).

_Theorem_ 2.8.1: Any `Lock` algorithm that, by reading and writing memory, solves deadlock-free mutual exclusion for three threads, must use at least three distinct memory locations.

                            
### Discussion Board Posts

- Why do we need to use mutual exclusion locks? What are the benefits and issues for using mutual exclusion locks?
  - We need mutual exclusion locks to enforce mutual exclusion in critical section and thus prevent deadlocks and starvation and even enforce the validity of results for many algorithms. The book succinctly states this in the following way: "without this property we cannot guarantee that a computation's results are correct."
  - The benefit of a mutual exclusion lock is the ability to ensure that only one thread is able to acquire the lock or token at a time. A downside or issue of this is that mutual exclusion locks that operation on $n$ threads must use at least $n$ memory addresses (Theorem 2.8.1) 
    
- We used contradiction to proof that LockOne does provide mutual exclusions. Could you use the similar approach to proof that LockTwo provides mutual exclusion?
  - Yes you can. The book actually uses a proof-by-contradiction to prove this very thing (See Lemma 2.3.2). I suspect proof-by-contradiction could be used to prove mutual exclusion of a Lock<N> class though this may be untenable for even modest values of $N$.

- Could you use Peterson lock to write a short program to show that it works?
```Java
class PetersonDemo implements Runnable {

  public static Peterson petersonLock;

  public static class Peterson {
    // thread-local index, 0 or 1
    private volatile boolean[] flag = new boolean[2];
    private volatile int victim;
    
    public void lock(int myId) {
      int i = myId;
      int j = 1 - i;
      flag[i] = true;                    // I’m interested
      victim = i;                        // but you go first
      while (flag[j] && victim == i) {}; // wait here
    }
    
    public void unlock(int myId) {
      int i = myId;
      flag[i] = false; // I’m not interested
    }
  }

  public void criticalSection() {
    // gets the name of current thread
    System.out.println("Current Thread Name: " + Thread.currentThread().getName());
    
    // gets the ID of the current thread
    System.out.println("Current Thread ID: " + Thread.currentThread().getId());
  }
  
  // Thread and main functions  
  public void run() {
    // threadID mod 2 
    int myId = (int) (Thread.currentThread().getId() % 2);

    petersonLock.lock(myId);
    criticalSection();
    petersonLock.unlock(myId);
  }
  
  public static void main(String[] args) {
    // Create static lock object
    petersonLock = new Peterson();

    // Runnable target
    PetersonDemo t = new PetersonDemo();

    // create threads
    Thread t1 = new Thread(t, "First Thread");
    Thread t2 = new Thread(t, "Second Thread");

    // start threads
    t1.start();
    t2.start();
  }
}
```

The output of the above code is: 

```
Current Thread Name: First Thread
Current Thread ID: 9
Current Thread Name: Second Thread
Current Thread ID: 10
```

If we comment out the `petersonLock` lines then we get the following output:
```
Current Thread Name: First Thread
Current Thread Name: Second Thread
Current Thread ID: 9
Current Thread ID: 10
```

## Module Three

### Notes

Same as [previous section](#module-two)

### Discussion Board Posts

We have discussed about two-thread mutual exclusion locks and n-thread mutual exclusion locks. From your opinions, what are the major differences in construction locks in these two categories of locks?

A major difference is the amount of space required. As noted in Theorem 2.8.1, by way of a three-thread lock, that any lock that provides deadlock-freedom must use at least three distinct memory locations. Thus for a two-thread lock such as the Peterson Algorithm, it must use two memory locations while an n-thread lock will require n memory locations. For large amounts of threads this can quickly become unwieldy to think about. Additionally, there is the consideration for the overflow in Lamport's Bakery Algorithm. One could imagine, however unlikely, the filter algorithm also overflowing. On a 32 bit system, it would only be possible to have $2^{32}$ threads. Neither of these are considerations for the two-thread lock.


When we talk about mutual exclusion locks, we consider exclusiveness, starvation, and fairness as major criteria for the effectiveness of locks. Are there other features or properties that we need to consider?

I can think of a few. First, the complexity of the algorithm. If the algorithm is too complex it may be hard to extend, scale, or even implement. Building off of complexity, we may not just care about source code or implementation complexity but also runtime and space complexity. We can care about a starvation free lock but starvation freedom is still a loose criteria. It implies deadlock freedom but does not imply that any thread that wants to acquire a lock will do so in a _reasonable_ amount of time. That is, we may care that given n threads, what is the average time to acquire a lock. 


Can you proof that using fewer than N MRMW registers can not provide mutual exclusions for N threads?
  
_Proof by contradiction_:

Assume that we can provide mutual exclusion for $N$ threads without an inconsistent state. That is, without loss of generality, there is some algorithm that uses $N-1$ registers or memory locations. 

Consider a MRMW location such as the Peterson Algorithm's `victim` array. This array will have only $N-1$ entries. Initially, the state $s$ is such that no thread has attempted to enter the critical section. Since our lock provides deadlock-free mutual exclusion thread $N$ enters the critical section without writing to `victim`. When the remainder of threads acquire the lock they write to `victim` at a unique index and provide a complete covering of `victim`. One of these $N-1$ threads will acquire the lock and enter the critical section because state of the lock is not tracking that thread $N$ entered the critical section. This contradicts our assumption that we could provide mutual exclusion without an inconsistent state.

Suppose that instead $N-1$ threads provide a covering of the `victim` memory locations and some thread $t$ is allowed to enter the critical section. Now suppose that thread $N$ wants to enter the critical section. As such, it writes to some location in $j$ in the `victim` array that it wants access to the critical section. The thread that originally marked location $j$ will proceed to enter the critical section since `victim[j] = N`. Thus there will be two threads in the critical section. This contradicts our original assumption. 

Thus, there is no way to provide mutual exclusion for $N$ threads without $N$ registers.

### Homework

#### Problem 1

Use Amdahl’s law to answer the following questions:

Suppose the sequential part of a program accounts for 40% of the program’s execution time on a single processor. Find a limit for the overall speedup that can be achieved by running the program on a multiprocessor machine.

$$
\begin{align*}
S &= \lim_{n\to\infty}\frac{1}{(1 - p) + \frac{p}{n}}     \\
  &= \lim_{n\to\infty}\frac{1}{(1 - 0.6) + \frac{0.7}{n}} \\
  &= \lim_{n\to\infty}\frac{1}{0.4 + \frac{0.7}{n}}      \\
  &= \frac{1}{0.4} \\
  &= 2.5
\end{align*}
$$


Now suppose the sequential part accounts for 30% of the program’s computation time. Let $s_{n}$ be the program’s speedup on n processes, assuming the rest of the program is perfectly parallelizable. Your boss tells you to double this speedup: the revised program should have speedup $s'n > 2sn$. You advertise for a programmer to replace the sequential part with an improved version that runs k times faster. What value of k should you require?

I spent significantly more time on this problem than I'm sure was intended. But I've derived a general result which I'm pleased with.

Frist, recognize that for a parallizable portion $\beta$ and serial portion $\alpha$ we have $1 - \alpha = \beta$. Second, for any optimization factor $\Delta$ on $\alpha$ we have the optimized runtime $\alpha'=\frac{\alpha}{\Delta}$. Note that when the optimization factor $\Delta = 1 \implies \alpha' = \alpha$.  

$$
\begin{align*}
s'_{n} > \Delta s_{n} &\iff \frac{1}{\frac{alpha}{k} + \frac{1 - \frac{\alpha}{k}}{n}} > \Delta(\frac{1}{\alpha + \frac{1 - \alpha}{n}})\\
 &\iff \frac{1}{\frac{\alpha n + k - \alpha}{kn}} > \frac{\Delta}{\frac{\alpha n + 1 - \alpha}{n}} \\
 &\iff \frac{kn}{\alpha (n-1) + k} > \frac{n\Delta}{\alpha (n-1) + 1} \\
 &\iff kn(\alpha (n-1) + 1) > n\Delta(\alpha (n-1) + k) \\
 &\iff kn\alpha (n-1) + kn > n\Delta\alpha (n-1) + nk\Delta \\
 &\iff kn\alpha (n-1) + kn - nk\Delta > n\Delta\alpha (n-1) \\
 &\iff kn(\alpha (n-1) + 1 -\Delta) > n\Delta\alpha (n-1) \\
 &\iff k(\alpha (n-1) + 1 -\Delta) > \Delta\alpha (n-1) \\
 &\iff k > \frac{\Delta\alpha (n-1)}{(\alpha (n-1) + 1 -\Delta)} \\
 &\iff k > \{n-1}{n-1}\frac{\Delta\alpha}{(\alpha + \frac{1 -\Delta}{n-1})} \\
 &\iff k > \frac{\Delta\alpha}{\alpha + \frac{1 -\Delta}{n-1}}
\end{align*}
$$

In the problem statement we have $\Delta = 2$ and $\alpha = 0.3$. Thus the value of $k$ one should aim for is:

$$
\begin{align*}
k & > \frac{2 * 0.3}{0.3 + \frac{1 - 2}{n-1}} \\
  & > \frac{0.6}{0.3 - \frac{1}{n-1}}
\end{align*}
$$

Naturally, this is a function of $n$ as we have the parallel portion of the code to consider. But, in this problem we were asked to consider perfect parallelization. So that leads our final result:

$$
\begin{align*}
k > \lim_{n\to\infty} \frac{0.6}{0.3 - \frac{1}{n-1}} = 2
\end{align*}
$$

Thus, we want to make the serial portion of the program $k>2$ times faster.

Suppose the sequential part can be sped up three-fold, and when we do so, the modified program takes half the time of the original on n processors. What fraction of the overall execution time did the sequential part account for? Express your answer as a function of n.

We have $k = 3$ and $\Delta = 0.5$, then:

$$
\begin{align*}
\Delta s'_{n} &= \frac{1}{\frac{\alpha}{k} + \frac{1 - \alpha}{n}} \\
      &= \frac{1}{\frac{n\alpha}{nk} + \frac{k(1 - \alpha)}{kn}} \\
      &= \frac{1}{\frac{n\alpha + k(1 - \alpha)}{kn}} \\
      &= \frac{kn}{n\alpha + k(1 - \alpha)}
\end{align*}
$$

Now we solve for $\alpha$:

$$
\begin{align*}
\Delta s_{n} = \frac{kn}{n\alpha + k(1 - \alpha)} &\iff \Delta s'_{n}(n\alpha + k(1 - \alpha)) = kn \\
 &\iff \Delta s_{n}(n\alpha + k - k\alpha)) = kn \\
 &\iff \Delta s_{n}n\alpha + \Delta s_{n}k - \Delta s_{n}k\alpha = kn \\
 &\iff \alpha(n\Delta s_{n} - \Delta s_{n}k) + \Delta s_{n}k  = kn \\
 &\iff \alpha = \frac{kn - \Delta s_{n}k}{n\Delta s_{n} - \Delta s_{n}k}
\end{align*}
$$

If we assume that $s_{n} = 1 \implies \Delta s_{n} = 2s_{n} = 2$ for the program taking _half_ as long. Recall that we sped the serial portion up by $k = 3$ fold. Thus:

$$
\begin{align*}
\alpha(n) &= \frac{kn - \Delta s_{n}k}{n\Delta s_{n} - \Delta s_{n}k} \\
        &= \frac{3n - (2)(3)}{2n - (2)(3)}\\
        &= \frac{3(n - 2)}{2(n - 3)}\\
        &= \frac{3}{2}\frac{n - 2}{n - 3}\\
\end{align*}
$$

#### Problem 2

You have a choice between buying one uniprocessor that executes five zillion instructions per second, or a ten-processor multiprocessor where each processor executes one zillion instructions per second. Using Amdahl’s Law, explain how you would decide which to buy for a particular application.

Amdahl's Law states the speedup $S$ can be given by:

$$
S = \frac{1}{1 - p + \frac{p}{n}}
$$

Let us simplify the problem by a factor of a _zillion_. That is, one uniprocessor executes 5 instructions per second while we have the option of a ten-processor where each core can execute 1 instruction per second.

Suppose our program can be either perfectly linear or perfectly parallel. In the former case we have:

$$
S_{u} = \frac{1}{1 - 0 + \frac{0}{1}} = 1
$$

In the latter case we have:
$$
S_{p} = \frac{1}{1 - 1 + \frac{1}{10}} = 10
$$

That's not too surprising. So, in the uniprocessor case we can expect our perfectly serial program to operate at 5 i/sec while the perfectly parallel application would operate at 10 i/sec. This is because in the former our processor works at 5 i/sec and the speedup is 1. Thus, the final i/sec is 5. In the later the processor speed is 1 i/sec and the speedup is 10 leading to 10 times 1 i/sec.

So what we need is to figure out what serial/parallel ration of our application would yield a greater than 5 speed up on the multiprocessor. If we solve:

$$
f(x) = \frac{1}{1 -\frac{x}{100} + \frac{\frac{x}{100}}{10}} > 5
$$

We reach this point at $x = 88.8888...$. With this in mind, I would purchase the multiprocessor _only if_ $~88.9%$ of my application were parallizable. If the portion of parallelization is less, then the uniprocessor is actually more beneficial.



#### Problem 3:

Programmers at the Flaky Computer Corporation designed the protocol shown below to achieve n-thread mutual exclusion. For each question, either sketch a proof, or display an execution where it fails.

Does this protocol satisfy mutual exclusion?
Is this protocol starvation-free?
Is this protocol deadlock-free?

The Flaky lock

```Java
class Flaky implements Lock {

   private int turn;
   private boolean busy = false;

   public void lock() {
      int me = ThreadID.get();

      do {
          do {
                  turn = me;
           } while (busy);
          busy = true;
      } while (turn != me);
   }

   public void unlock() {
       busy = false;
   }
}
```

##### _Does this protocol satisfy mutual exclusion?_

Yes.

_Proof by contradiction_:
Suppose the algorithm does not satisfy mutual exclusion. Then $\exists i,j \in \mathbb{Z}: CS_{A}^{i} \nprec CS_{B}^{j} \wedge CS_{B}^{j} \nprec CS_{A}^{i}$. 

Consider before each threads last execution of lock before entering its $i^{th}(j^{th})$ critical section. Inspecting the code we see that:

$$
\begin{equation}
write_{A}(turn=A) \rightarrow read_{A}(busy) \rightarrow write_{A}(busy=true) \rightarrow read_{A}(turn \neq A)
\end{equation}
$$

$$
\begin{align}
write_{B}(turn=B) \rightarrow read_{B}(busy) \rightarrow write_{B}(busy=true) \rightarrow read_{B}(turn \neq B)
\end{align}
$$

Thread B must assign 'true' to _busy_ between events $write_{A}(turn = A)$ and $read_{A}(turn \neq A)$. Since this is the last assignment we have:

$$
\begin{align}
write_{A}(turn = A) \prec write_{B}(turn = B) \prec read_{A}(turn \neq A)
\end{align}
$$

Once _turn_ is set by thread B, it does not change, so any subsequent read returns B, contradicting the second equation. Therefore the algorithm is mutually exclusive.

##### _Is this protocol starvation-free?_

No, it is not starvation free. The next answer has a proof that this algorithm is not deadlock free. Thus it must not be starvation free.

##### _Is this protocol deadlock-free?_

This algorithm is not deadlock free. The following proof by contradiction shows that a thread can get deadlocked. That is, it never acquires the lock and this is _not_ due to other threads completing an infinite number of critical sections.

I actually used the demo code from a previous module discussion to run this implementation of a lock with multiple threads. For thread counts greater than one, there was always one thread that would get deadlocked. That is it would not enter the critical section (a series of print statements).

_Proof by contradition_: Suppose that the lock is starvation-free. Then any thread that attempts to acquire the lock will eventually acquire it and enter the critical section. Lets consider the last execution of the lock for threads A and B.

$$
\begin{equation}
write_{A}(turn=A) \rightarrow read_{A}(busy) \rightarrow write_{A}(busy=true) \rightarrow read_{A}(turn \neq A)
\end{equation}
$$

$$
\begin{align}
write_{B}(turn=B) \rightarrow read_{B}(busy) \rightarrow write_{B}(busy=true) \rightarrow read_{B}(turn \neq B)
\end{align}
$$

With out loss of generality, assume that A was the last to write to _busy_ and that thread A read the value of _busy_ before thread B wrote to it. That is:

$$
\begin{align}
write_{B}(busy=true) \rightarrow write_{A}(busy=true)
\end{align}
$$

And

$$
\begin{align}
read_{A}(busy=false) \rightarrow write_{B}(busy=true)
\end{align}
$$

Then we have:

$$
\begin{align}
read_{A}(busy=false) \rightarrow write_{B}(busy=true) \rightarrow write_{A}(busy=true)
\end{align}
$$

This is possible if thread A reads _busy_ then is suspended by the operating system or otherwise pre-empted since _busy_ is not an atomic register. This is a race condition within the lock algorithm as implemented above. Now we turn to the temporal ordering of accesses to _turn_.

Without loss of generality, assume that thread A was the last to write to _turn_:

$$
\begin{align}
write_{B}(turn=B) \rightarrow write_{A}(turn=A)
\end{align}
$$

The above relation implies the following two relations:

$$
\begin{align}
write_{A}(busy=true) \rightarrow read_{A}(turn=A)
\end{align}
$$

$$
\begin{align}
write_{B}(busy=true) \rightarrow read_{B}(turn=A)
\end{align}
$$

By the transitivity property we have:

$$
\begin{align}
write_{B}(busy=true) \rightarrow write_{A}(busy=true) \rightarrow read_{A}(turn=A)
\end{align}
$$

$$
\begin{align}
write_{B}(busy=true) \rightarrow write_{A}(busy=true) \rightarrow read_{B}(turn=A)
\end{align}
$$

This is a contradiction since thread A will never enter the critical section as there are no intervening writes to _turn_ that would alter its value. Thus thread A will be deadlocked. Therefore this algorithm is not deadlock free or starvation free.





#### Problem 4

In practice, almost all lock acquisitions are uncontended, so the most practical measure of a lock’s performance is the number of steps needed for a thread to acquire a lock when no other thread is concurrently trying to acquire the lock.

Scientists at Cantaloupe-Melon University have devised the following “wrapper” for an arbitrary lock, shown below. They claim that if the base Lock class provides mutual exclusion and is starvation-free, so does the FastPath lock, but it can be acquired in a constant number of steps in the absence of contention. Sketch an argument why they are right, or give a counterexample.

```Java
class FastPath implements Lock {

 private static ThreadLocal<Integer> myIndex;
 private Lock lock;
 private int x, y = -1;

 public void lock() {
     int i = myIndex.get();
     x = i;                       // I’m here
     while (y != -1) { }          // is the lock free?
      y = i;                      // me again?
     if (x != i)                  // Am I still here?
        lock.lock();              // slow path
  }
  
  public void unlock() {
     y = -1;
     lock.unlock();
  }
}
```


If there is any contention in the lock, all threads but one, say A, will need to acquire the underlying lock. This would mean that one thread would acquire the resource while the remaining are subject to a mutually exclusive and starvation free lock. However, the underlying lock does not seem to share state with the wrapper. Thus, without loss of generality, the first lock, say B, to enter the underlying lock will acquire it and proceed into the critical section. This is not mutually exclusive if there is any contention!

In the absence of contention this lock-wrapper can be acquired in a constant number of steps. Consider the last thread, say B, to ever acquire the lock in the execution of some process (a case of uncontended acquisition). Thread A will enter the wrapper and set _x_ to its thread id. Thread B will loop in the `while (y != -1){}` line if a thread currently has the lock. Thread B may now set $y = tid(B)$. Thread B then proceeds to the `if (x !=1)` statement. Since Thread B is the last thread to acquire the lock, there are no writes to _x_ between $write_{B}(x = B)$ and $read_{B}(x \neq B)$. As a result of this it will enter the fast path of lock acquisition (i.e. proceed directly to the critical section). This is a constant number of steps.

In the case that some thread, say A, already has the lock, there will be a non-constant number of steps to acquire the lock. That is, the while loop will step as a function of the time thread A spends in the critical section. 

There is a race condition between thread A reading the value _x_ and some thread B writing to it. That is $write_{A}(x = A) \prec write_{B}(x = B) \prec read_{A}(x)$. But that is contended access.



These exercises must be completed and submitted no later than Day 7 of Module 5.

## Module Four

### Notes

Reading:
  Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
  Chapter 3: Concurrent Objects

_Principle 3.3.1_: Method calls should appear to happen in a one-at-a-time sequential order.

Definition: An object is _quiescent_ if it has no pending method calls.

_Principle 3.3.2_: Method calls separated by a period of quiescence should appear to take affect in their "real-time" order.

Together Principles 3.3.1 & 3.3.2 define a correctness property called _quiescent consistency_. This is a _nonblocking_ correctness condition as well.

A correctness property $\mathcal{P}$ is _compositional_ $\iff \forall \theta \in \mathcal{S}, \theta \in \mathcal{P}$. That is to say, if every object $\theta$ in a system $\mathcal{S}$ has property $\mathcal{P}$ then $\mathcal{S}$ has $\mathcal{P}$.

A method on an object is _total_ if it is well-defined for every state of the object. Otherwise it is _partial_. For example pushing on to an infinite stack is a _total_ method. However, popping is _partial_ because one can not pop an object from an empty stack.

_Principle 3.4.1_: Method calls should appear to take effect in program order.

Principle 3.4.1 together with Principles 3.3.1 & 3.3.2 define the correctness property called _sequential consistency_, which is widely used in the literature on multiprocessor synchronization.

Note that quiescent consistency and sequential consistency are _incomparable_. That is, it is possible to have one property without the other.

The concepts of _barrier_ and _fence_ are used to enforce sequential consistency between two regions: be that network traffic or process execution. By that I mean, in the case of process execution, that if we have two add instructions, a barrier, followed by two subtraction instructions then we can be sure that the two subtraction instructions will follow the two addition instructions. We cannot, however, be sure of the order of the two addition instructions prior to the barrier. Similarly, we cannot be sure of the ordering of the two subtraction instructions following the barrier. See [memory models](https://www.cs.utexas.edu/~bornholt/post/memory-models.html) for more information.


Sequential consistency is not compositional.

_Principle 3.5.1_: Each method call should appear to take effect instantaneously at the some moment between its invocation and its response.

The above principle states that the "real-time" behavior of method calls must be preserved. This correctness property is called _linearizability_. Linearisability is a stronger property than sequential consistency thus every linearisable execution is sequentially consistent but not every sequentially consistent execution is linearisable. The usual way to show that a concurrent object implementation is linearisable is to identify for each method a _linearisation point_ where the method takes effect. For lock-based implementations, each method's critical section can serve as its linearisation point. For implementations which do not make use of locks, the linearisation point is typically a single step at which point the side effects of the method call are visible or available to other method calls. The property linearisability is compositional.  
                                                                     
The execution of a concurrent system is modeled by by a _history_, a finite sequence of method invocation and response events. A _subhistory_ of a history $\mathcal{H}$ is a subset of sequences in $\mathcal{H}$. We write the invocation of a method as $\langle x.m(a*) A \rangle$ for method $m$ on object $x$ in thread $A$ with arguments $a*$. Similarly, a response event is $\langle x:t(r*) A \rangle$ where $t$ is either $OK$ or an exception name and $r*$ is a sequence of result values.

A _method call_ is a pair consisting of an invocation and its matching response both in a history $\mathcal{H}$. An invocation is _pending_ in $\mathcal{H}$ if no matching response following the invocation. An _extension_ of a history $\mathcal{H}$ by history $\mathcal{H}'$ is $\mathcal{H}$ appended with $\mathcal{H}'$. A history is said to be complete if it contains no pending invocations.

A  _subhistory_ $\mathcal{H} | C$,  ($\mathcal{H}$ at $C$), for history $\mathcal{H}$ and some class of events $C$, is a subsequence of all events in $\mathcal{H}$  whose thread context is compromised of the object class $C$. The object class can be a thread or object. Two history are equivalent if and only if $\mathcal{H} | A = \mathcal{H}' | A, \forall A \in \mathcal{H}$. A history is _well-formed_ if every thread subhistory in the history is sequential.

A _sequential specification_ for an object is a set sequential histories for the object. A sequential history is _legal_ if each object subhistory is legal for that object. This definition given by the book is recursive and I don't think it makes much sense. 

Recall that a partial order $\rightarrow$ is irreflexive and transitive. That is some event $x$ cannot precede itself (irreflexive). A _total order_ "$<$" on $\mathcal{X}$ is a partial order such that $\forall x,y\in\mathcal{X}, x \neq y$ and $(x < y)$ or $(y < x)$.

_Fact 3.6.1_: If $\rightarrow$ is a partial order on $\mathcal{X}$, then there exists a relation $<$ on $\mathcal{X}$ such that $x,y\in\mathcal{X}: x \rightarrow y \implies x < y$.

For methods $m_{0}, m_{1}$ and history $\mathcal{H}$, we say that $m_{0} \rightarrow_{\mathcal{H}} m_{1}$ if $m_{0}$ precedes $m_{1}$ in $\mathcal{H}$.

_Definition 3.6.1_: A history $\mathcal{H}$ is linearisable if it has an extension $\mathcal{H}'$ and there is a legal sequential history $\mathcal{S}$ such that:
- complete($\mathcal{S}) = \mathcal{S}$
- if $m_{0} \rightarrow_{\mathcal{H}} m_{1} \implies  m_{0} \rightarrow_{\mathcal{S}} m_{1}$

Note that a history may have many linearisation.

_Theorem 3.6.1_: $\mathcal{H}$ is linearisable $\iff \forall x\in\mathcal{H}, \mathcal{H}|x$ is linearisable.

Linearisability is a _nonblocking_ property: a pending invocation of a total method is never required to wait for another pending invocation to complete.

_Theorem 3.6.2_: Let $inv(x)$ be an invocation of a total method. If $\langle x\quad inv\quad P \rangle$ is a pending invocation in a linearisable history $\mathcal{H}$, then $\exists \langle x\quad res\quad P \rangle :\quad \mathcal{H}\cdot\langle x\quad res\quad P \rangle$ is linearisable.

A call is _wait-free_ if will always finish execution in a finite number of steps.

A call is _lock-free_ if guarantees that infinitely often some method call finishes in a finite number of steps. Note that this implies that some thread(s) may starve. Additionally, wait-free implies lock-free. 

_Definition 3.7.1_: A method is _obstruction-free_ if, from any point after which it executes in _isolation_, that is no other threads take steps concurrently, it will finish in a finite number of steps.


### Discussion Board Posts

In this module, we talked about linearisability and sequential consistency. Both of them belongs to concurrent specifications. What are the purposes of concurrent specifications? Why do we need them?

Concurrent specifications are a set of histories of some (class of) objects. That is, they are a set of execution traces for some object in the context of a thread or process. To use the notation of the book, they are H|x for some history H and object or method x. Why do we need them? They help us determine the properties of a method or object. Using a history we can see if a particular object satisfies linearization or sequential consistency, or even some other property of condition. Additionally, they may be useful for building systems. Knowing if a method or object is composable which may inform its design.


In the linearisability analysis, we always exam the sequence of thread executions over concurrent objects and decide if it can be considered as linearisability. Why do we care of this? Why linearisability is important?

Linearisability is composable whereas sequential consistency is not. That is if we can guarantee that all parts of a system are linearisable then we guarantee the system as a whole is linearisable. Linearisation is desirable because it is a nonblocking property. This means that any invocation of a (total) method is not required to wait for some other pending invocation. This is a very nice property to have in concurrent systems.


In our lecture, there is an example that a history of concurrent objects can be considered as sequential consistency but not linearisability. What are the meanings of that? Can we consider another history that is linearisability but not sequential consistency, and explain your conclusions.

One meaning of that could be that they may be blocking. Sequential consistency has no requirement for blocking. Where as linearisation does. Additionally, linearisation requires that if some method precedes another in a history then that relation will be respected in the system of execution. Sequential consistency does not have this property (composition). Further, sequential consistency does not imply linearisation.

We cannot consider a history that is linearisable but not sequentially consistent for the reason stated in the previous sentence. 


## Module Five

### Notes

Reading:
  Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
  Chapter 3: Concurrent Objects
  
_Church-Turing Thesis_: Anything that _can_ be computed, can be computed by a Turing Machine.

A _read-write register_, is an object that encapsulates a value that can be observed by a `read()` method and modified by a `write()` method-- load and store, respectively.

Registers can be classified according to the [consistency condition](https://en.wikipedia.org/wiki/Consistency_model) they satisfy when accessed concurrently, the domain of possible values that can be stored, and how many processes can access with the _read_ or _write_ operation

| Consistency Condition | Domain          | Write            | Read             |
|-----------------------|-----------------|------------------|------------------|
| Safe, Regular, Atomic | Binary, Integer | Single, Multiple | Single, Multiple |

A _single-writer, multi-reader_ register implementation is _safe_ if:
- A load operation not concurrent with any store operation returns the value written by the last store operation.
- A load operation that is concurrent with a store operation may return any value within the registers domain (boolean, integer, etc).

Any register implementation, whether safe, regular, or atomic, defines a total order on the store operations called the _write order_, the order in which stores/writes "take affect" in the register.

A _regular register_ fulfils the following conditions for some history $\mathcal{H}$:
- $\forall i\in\mathcal{H}: \mathcal{R}^{i} \nrightarrow \mathcal{W}^{i}$
- $\forall i,j\in\mathcal{H}: i \rightarrow j \implies \neg(\mathcal{W}^{i} \rightarrow \mathcal{W}^{j} \rightarrow \mathcal{R}^{i})$

In short, the read of some value cannot precede a write of that value and intervening writes cannot be dropped or ignored by read.

A regular register is safe and quiescently consistent.

A _collect_ is the non-atomic act of copying the register values one-by-one into an array. If we perform two collects one after the other, and both collects read the same set of timestamps or values, then we know there was an interval during which no thread updated its register, so the result of the collect is a snapshot of the system state immediately after the end of the first collect. We call such a pair of collects a _clean double collect_.

An _atomic register_ is a linearisable implementation of a registers. It satisfies one additional condition:
- $\mathcal{R}^{i} \rightarrow \mathcal{R}^{j} \implies i \leq j, i,j\in\mathcal{H}$


The below provides a single-writer atomic snapshot class with `collect()`, `update()`, & `scan()` methods.
```Java
public class WFSnapshot<T> implements Snapshot<T> {
  private StampedSnap<T>[] a_table; // array of atomic MRSW registers
  public WFSnapshot(int capacity, T init) {
    a_table = (StampedSnap<T>[]) new StampedSnap[capacity];
    for (int i = 0; i < a_table.length; i++) {
      a_table[i] = new StampedSnap<T>(init);
    }
  }

  private StampedSnap<T>[] collect() {
    StampedSnap<T>[] copy = (StampedSnap<T>[])
    new StampedSnap[a_table.length];
    for (int j = 0; j < a_table.length; j++)
     copy[j] = a_table[j];
    return copy;
  }
 
  public void update(T value) {
    int me = ThreadID.get();
    T[] snap = scan();
    StampedSnap<T> oldValue = a_table[me];
    StampedSnap<T> newValue =
    new StampedSnap<T>(oldValue.stamp+1, value, snap);
    a_table[me] = newValue;
  }

  public T[] scan() {
    StampedSnap<T>[] oldCopy;
    StampedSnap<T>[] newCopy;
    boolean[] moved = new boolean[a_table.length];
    oldCopy = collect();
    collect: while (true) {
    newCopy = collect();
    for (int j = 0; j < a_table.length; j++) {
      if (oldCopy[j].stamp != newCopy[j].stamp) {
        if (moved[j]) {
         return oldCopy[j].snap;;
        } else {
          moved[j] = true;
          oldCopy = newCopy;
          continue collect;
        }
      }
    }
    T[] result = (T[]) new Object[a_table.length];
    for (int j = 0; j < a_table.length; j++)
    result[j] = newCopy[j].value;
    return result;
  }
}
```



_Lemma 4.3.1_: If a scanning thread makes a clean double collect, then the values it returns where the values that existed in the registers in some state of the execution.

_Lemma 4.3.2_: If a scanning thread $A$ observes changes in another thread $B$'s label during two different double collects, then the value of $B$'s register read during the last collect was written by and `update()` call that begin after the first of the four collects started.

_Lemma 4.3.3_: The values returned by a `scan()` were in the registers as some state between the calls invocation and response.

_Lemma 4.3.4_: Every `scan()` or `update()` returns after at most $O(n^{2})$ reads or writes.


### Discussion Board Posts

In this module, we talked about safe register memory type. It has some interesting characters. From your experiences, have you observed other systems (e.g. electrical, control, hydraulic, fluid dynamic, etc.) have the similar phenomena as the safe register? What is your understanding of the safe register concept?

A register can be deemed safe if it satisfies the following conditions:
- Only a single thread may execute a store operation at any given time unit. There may be multiple threads executing a load operation at any given time unit.
- A load operation not concurrent with any store operation returns the value written by the last store operation.
- A load operation that is concurrent with a store operation may return any value within the registers domain (boolean, integer, etc).

I can only think of one case though I'm very interested to see what others come up with. Both are a bit of a stretch I'd say. The first is compiler undefined behavior. Imagine condition two from above as the language specification (C for example) which defined a set of allowed operations, type interactions, semantics, and syntax. If we operate outside of that specification then we get undefined behavior. The undefined behavior can be implemented in any way within the domain of the underlying machine code and system (assembly & operating system). In this last way, we see that a load operation concurrent with a store operation is undefined and may yield anything within the domain. 
    
    
If you only have regular register memory type, what can you do to guarantee that the system results are linearisable?

Since a regular register is only defined for single-writer, the _write order_ is trivial. Thus we need only to ensure that a read does not "flicker" between values.

We can perform a double collect at each read. If the result is a clean double collect then we can be sure there were no writes in the interval of reading-- of the double collect-- and we have a well defined precedes relationship between the last write and current read with no intervening write. If we don't have a clean double collect we discard the read value and try again. This would get us condition 4.1.3 from the text and therefore linearization. Though, I don' think this would get us a useful read method.


In our lecture, we used time stamp in the memory road map build. Can we replace time stamp with label to realize the same purposes? Please give your thoughts.

I'm not sure what the "memory road map build" refers to so I'll take my best guess and assume it just means a shared memory register implementation around time stamp and label/snapshot based implementations in the lecture notes.

I believe one can, yes, on the condition that the labels monotonically increase. Then the labels are an ordered set and can serve the same function as a time stamp. However, in both cases we have the risk of an overflow at the end of the range. In the lecture PDF the label is give as the high $2^{8}$ bits meaning that particular implementation can support only 256 ordered writes or snapshots. 


## Module Six

### Notes
Herlihy, M. & Shavit, N. (2021). The art of multiprocessor programming, 2nd Ed.
Chapter 5: The Relative Power of Primitive Synchronization Operations

_Synchronization Primitive_: A set of method or functions that map directly to primitive synchronization instructions.

_Hierarchy of Synchronization Primitives_: A set of classes each of which has an associated _consensus number_, which is the maximum number of threads for which objects of the class can solve an elementary synchronization problem called _consensus_.

The hierarchy is not _robust_. That is, whether an object $X$ at level $m$ can be raised to a higher consensus level by combining it with another object $Y$ at the same or lower level. Informally, let $X$ be an object with the following curious properties. $X$ solves $n$-thread consensus but refuses to reveal the results unless the caller can prove they can solve an intermediate task weaker than $n$-thread consensus but strong than any task solvable by atomic read/write registers. If $Y$ is an object that can be used to solve the intermediate task, then $Y$ can raise $X$ by convincing $X$ to reveal the outcome of an $n$-thread consensus. These object are non-deterministic. 

_Consensus_: A problem in which each participating thread calls a function `decide()` with an input $v$ _at most once_. The function will return a value meeting the following conditions:
- _consistent_: all threads decide the same value,
- _valid_: the common decision value is some thread's input.
A concurrent consensus problem is linearizable to a sequential consensus problem in which the thread whose value was chosen completed its `decide()` function call first.

_Consensus Protocol_: An implementation of consensus which is wait-free.

_Definition 5.1.1_: A class $C$ _solves n_-thread consensus if there exist a consensus protocol using any number of objects of class $C$ and any number of atomic registers.

_Definition 5.1.2_: The _consensus number_ of a class $C$ is the largest $n$ for which that class solves $n$-thread consensus. If no largest $n$ exists, we say the consensus number of the class is _infinite_.

_Corolloary 5.1.1_: Suppose one can implement an object of class $C$ from on or mor objects of class $D$, together with some number of atomic registers. If class $C$ solves $n$-consensus, then so does class $D$.

_Protocol State_: A set of stated of threads, the shared objects, and a set of state transitions called _moves_ in which a thread calls the shared object.

An _initial state_ is a protocol state before any thread has moved, and a _final state_ is a protocol state after all threads have finished. The _decision value_ of any final state is the value decided by all threads in that state. A protocol state is said to _bivalent_ if the decision value is not yet fixed. By contrast, a _univalent_ protocol state is one in which the decision value is fixed.

_Lemma 5.1.1_: Every 2-thread consensus protocol has a bivalent initial state.

The above lemma means that the result of two threads interacting with a shared object cannot be known before hand. That is, the final result depends on the interleaving of the reads and writes of the two threads.

_Lemma 5.1.2_: Every $n$-thread consensus protocol has a bivalent initial state.

A protocol state is _critical_ if:
- it is bivalent, and
- if any thread moves, the protocol state becomes univalent.

_Lemma 5.1.3_: Every wait-free consensus protocol has a critical state.

Lemma 5.1.3 states that if a consensus problem is wait-free then there will eventually be a protocol state transition from bivalent to  univalent state.

_Theorm 5.2.1_: Atomic registers have consensus number 1.

_Corollary 5.2.1_: It is impossible to construct a wait-free implementation of any object with consensus number greater than 1 using atomic registers.

_Theorem 5.4.1_: The two-dequeuer First-In-First-Out (FIFO) queue class has consensus number at least $2$.

_Corollary 5.4.1_: It is impossible to construct a wait-free implementation of a queue, stack, priority queue, set, or list from a set of atomic registers.

_Theorem 5.4.1_: FIFO queues have consensus number $2$.

An $(m,n-assignment) problem, for $n \geq m > 1$, is one in which we are given an object with $n$ fields and assigns $m$ values $v_{i},i\in[0,...,m-1]$ and $m$ index values $i_{j},j\in[0,...,m-1]$, $i_{j}\in[0,..., n-1]$. It atomically assigns $v_{j}$ to array element $i_{j}$. 

_Theorem 5.5.1_: There is no wait-free implementation of an $(m,n)$-assignment object by atomic registers for any $n>m>1$.

_Theorem 5.5.2_: Atomic $(n,\frac{n(n+1)}{2})$-register assignments for $n>1$ has a consensus number at least $n$.

Many classical synchronization operations provided by multiprocessors in hardware can be expressed as _read-modify-write_ (RMW) operations/registers.

A method is RMW  for the function set $F$ if it atomically replaces the current register value $v$ with $f(v)$, for some $f \in F$, and returns the original value $v$. Notes that $F$ may be a singleton set. The following are some RMW operations:
- `fetch_and_set(v)`: $f_{v}(x) = v$.
- `fetch_and_incrememt()`: $f(x) = x + 1$.
- `fetch_and_add(k)`: $f_{k}(x) = x + k$.
- `compare_and_set()`: $f_{e,u}(x) = x \implies x \neq e \vee u$, for expected value $e$ and update value $u$.
- `fetch()`: $f(v) = v$, this is the RMW identity function.

An RMW operation is _non-trivial_ if its set of functions includes as least one non-identity function.

_Theorem 5.6.1_: Any non-trivial RMW register has consensus number at least $2$.

_Corollary 5.6.1_: It is impossible to construct a wait-free implementation of non-trivial RMW method from atomic registers for two or more threads.

_Definition 5.7.1_: $F\in Common2 \implies \forall f_{i},f_{j}\in F: f_{i} \circ f_{j} = f_{j} \circ f_{i}\quad \vee\quad f_{i} \circ f_{j} = f_{i}$

In English, $F$ is in _Common2_ if function composition commutes or $f_{i}$ overwrites $f_{j}$ in function composition.

_Definition 5.7.2_: A RMW register belongs to _Common2_ if its set of functions $F$ belongs to _Common2_.

_Theorem 5.7.1_: Any RMW register in _Common2_ has consensus number $2$.

_Theorem 5.8.1_: A register providing `compare_and_set()` and `fetch()` operations has an infinite consensus number.

_Corollary 5.8.1_: A register providing only `compare_and_set()` has an infinite consensus number.
      

### Discussion Board Posts
Explain your understanding of the concept of consensus. Why is this concept important for parallel computing?


We know that if you want to implement an object with consensus number X, you can’t use objects with consensus number Y, where Y < X.  On the other hand, the RMW objects use atomic registers in their codes. Are these two concepts contradicting to each other? 


In our lecture, we have introduced RMW objects and some of them have “commute” and “overwrite” properties. Can you suggest which RMW objects have these properties? 
